# ü§ñ AI AGENT BLUEPRINT: ENTERPRISE TEST AUTOMATION

**OPTIMIZED FOR:** Google Antigravity (Advanced AI Agent)  
**FRAMEWORK:** Playwright + TypeScript  
**APPROACH:** Incremental, Artifact-Based, Browser-Powered

---

## üìã APPLICATION CONFIGURATION

> [!IMPORTANT]
> **Specify the application URL before starting.** The AI agent will use this for exploration and testing.

**Application URL:** `https://www.littmann.in/3M/en_IN/littmann-stethoscopes-in/`

---

## üéØ AGENT CAPABILITIES OVERVIEW

As Google Antigravity, you have powerful capabilities that should be leveraged:

* **Browser Automation:** Use `browser_subagent` tool to explore applications, capture screenshots, and record interactions
* **Parallel Execution:** Create multiple files simultaneously when there are no dependencies
* **Artifact System:** Create structured documentation in the artifact directory for user review
* **Incremental Generation:** Generate code module-by-module to avoid token limits
* **Visual Proof:** Capture screenshots and recordings to demonstrate test execution

---

## üìã WORKFLOW PHILOSOPHY

**CRITICAL PRINCIPLES:**

1. **Explore First, Code Later:** Never write tests without thoroughly understanding the application
2. **Incremental Everything:** Generate one module at a time, one test at a time
3. **User Checkpoints:** Stop and ask for confirmation at key decision points
4. **Artifact-Centric:** Document all plans and progress in artifacts for transparency
5. **Visual Validation:** Use browser recordings and screenshots as proof of work
6. **Parallel When Safe:** Create multiple files simultaneously if no dependencies exist

---

## üìÖ PHASE 1: DISCOVERY & ANALYSIS (Browser-Powered)

**Goal:** Deeply understand the Application Under Test (AUT) through automated exploration.

### Step 1.1: Browser-Based Application Exploration

> [!NOTE]
> Use the **Application URL** from the [Project Configuration](#-project-configuration) section above.

**Use the `browser_subagent` tool to:**

1. **Navigate the Application:**
   ```
   Task: "Open [APPLICATION_URL from config], explore all navigation menus, click through major sections,
   capture screenshots of each page, and return a summary of all discovered pages and features."
   ```

2. **Capture Visual Documentation:**
   - Take screenshots of key pages (login, dashboard, main features)
   - Record user workflows (login flow, CRUD operations)
   - Note dynamic elements, modals, popups

3. **Identify UI Elements:**
   - Headers, navigation bars, sidebars
   - Forms, buttons, inputs, dropdowns
   - Tables, grids, lists
   - Modals, dialogs, notifications

### Step 1.2: Application Structure Mapping

Create a structured map of the application (can be in artifact or notes):

```markdown
## Application: [Name]

### Authentication
- Login page: /login
- Logout: Header dropdown
- Session timeout: 30 minutes

### Main Navigation
- Dashboard: /dashboard (landing page)
- [Feature 1]: /feature1 (list, create, edit, delete)
- [Feature 2]: /feature2 (specific workflows)
- Settings: /settings (user preferences, admin)

### Key Workflows
1. Login ‚Üí Dashboard ‚Üí [Core Action] ‚Üí Logout
2. [Feature-specific workflow]

### Dynamic Elements
- Modals: Confirmation dialogs, forms
- Notifications: Toast messages, alerts
- Loading states: Spinners, skeleton screens
```

### Step 1.3: Module Discovery (Application-Specific)

> [!IMPORTANT]
> **DO NOT use predefined module templates.** Discover modules based on ACTUAL application structure.

**Process:**

1. **Identify Functional Areas** from exploration
2. **Group Related Features** into logical modules
3. **Estimate Test Case Counts** per module (see AI_TEST_STANDARDS.md Section 11.3)
4. **Prioritize Modules** (Critical ‚Üí High ‚Üí Medium ‚Üí Low)

**Example Output:**

```markdown
## Discovered Modules

- Module 1: Authentication & Session Management (15 test cases) - Critical
- Module 2: Patient Registration & Search (14 test cases) - Critical
- Module 3: Appointment Scheduling (12 test cases) - High
- Module 4: Clinical Visits & Forms (10 test cases) - High
- Module 5: Reports & Analytics (8 test cases) - Medium
```

### Step 1.4: Architecture Planning (Component Object Model)

**Identify:**

* **Global Components:** Header, Navbar, Sidebar, Footer (reused across pages)
* **Shared Components:** Modals, DataTables, DatePickers, SearchBoxes
* **Page Objects:** Full page representations (LoginPage, DashboardPage, etc.)

**Output:** Mental model or document for COM structure

---

## üìù PHASE 2: TEST PLANNING (Incremental & Artifact-Based)

**Goal:** Create comprehensive test documentation WITHOUT writing code yet.

### Step 2.1: Create Test Plan Artifact

**File:** `testplan.md` (high-level strategy document)

**Content:**
- Introduction and scope
- Test modules with TC ID ranges
- Test summary table
- Execution strategy
- Entry/Exit criteria
- Risks and mitigation

**Reference:** AI_TEST_STANDARDS.md Section 10.2 for structure

### Step 2.2: Generate Test Cases (Module-by-Module)

> [!CAUTION]
> **NEVER generate all test cases at once.** This WILL exceed token limits.

**Incremental Workflow:**

1. **Generate Module 1 ONLY** in `testcases.md`
   - Use enterprise table format (see AI_TEST_STANDARDS.md Section 10.3)
   - Include: TC ID, Title, Priority, Type, Preconditions, Steps, Expected Results, Test Data

2. **STOP and ask user:**
   ```
   ‚úÖ Module 1: Authentication (TC001-TC015) generated successfully.
   
   Next modules to generate:
   - Module 2: [Name] (X test cases)
   - Module 3: [Name] (X test cases)
   
   Which module should I generate next? (or type 'all' to continue with remaining modules)
   ```

3. **Generate next module** after user confirmation
   - Read existing `testcases.md` to find last TC ID
   - Append new module (do NOT regenerate entire file)
   - Continue sequential numbering

4. **Repeat** until all modules complete

5. **Finalize** with test case summary table

**Reference:** AI_TEST_STANDARDS.md Section 11.2-11.3 for detailed workflow

---

## üíª PHASE 3: IMPLEMENTATION (Parallel & Efficient)

**Goal:** Write clean, maintainable Playwright code following strict standards.

### Step 3.1: Read Standards (CRITICAL)

**Before writing ANY code:**
- Read `AI_TEST_STANDARDS.md` completely
- Pay special attention to:
  - Locator Strategy (Section 1)
  - Component Object Model (Section 2)
  - Fixtures & Dependency Injection (Section 3)
  - Authentication & Storage State (Section 7)
  - Wait Strategies (Section 8)

### Step 3.2: Setup Project Structure (Parallel Creation)

**Create folders and base files simultaneously:**

```typescript
// Create in parallel (no dependencies):
- src/components/
- src/pages/
- src/tests/
- src/fixtures/custom-test.ts
- src/api/ (if needed)
- playwright.config.ts
- .env.example
```

### Step 3.3: Implement Component Object Model (Bottom-Up)

**Order of Implementation:**

1. **Shared Components First** (can be created in parallel):
   - `src/components/Header.ts`
   - `src/components/Navbar.ts`
   - `src/components/Modal.ts`
   - `src/components/DataTable.ts`

2. **Page Objects Second** (compose components):
   - `src/pages/LoginPage.ts` (uses Header if applicable)
   - `src/pages/DashboardPage.ts` (uses Header, Navbar, etc.)
   - `src/pages/[Feature]Page.ts`

3. **Update Fixtures** (`src/fixtures/custom-test.ts`):
   - Import all page objects
   - Add to `MyFixtures` type
   - Initialize in `test.extend()`

**Reference:** AI_TEST_STANDARDS.md Section 2 for COM architecture

### Step 3.4: Implement Tests (One Module at a Time)

**Module-by-Module Approach:**

1. **Select Module 1** (e.g., Authentication)

2. **Create test file:** `src/tests/auth/auth.spec.ts`

3. **Implement ONE test at a time:**
   ```bash
   # Write TC001
   # Run ONLY TC001: npx playwright test --grep="TC001"
   # Fix if needed
   # Move to TC002 only after TC001 passes
   ```

4. **STOP after module complete** and ask user:
   ```
   ‚úÖ Module 1: Authentication tests (TC001-TC015) implemented and verified.
   
   Which module should I implement next?
   ```

5. **Repeat** for next module

**Benefits:**
- Easier debugging (one test at a time)
- Faster feedback loop
- Prevents cascading failures
- Builds confidence incrementally

**Reference:** AI_TEST_STANDARDS.md Section 10 for checklist

### Step 3.5: Authentication & Global Setup

**Setup Project:**

1. **Create `tests/auth.setup.ts`** for storage state
2. **Configure `playwright.config.ts`** with setup project
3. **Override storage state** for auth tests:
   ```typescript
   // In auth.spec.ts
   test.use({ storageState: { cookies: [], origins: [] } });
   ```

**Reference:** AI_TEST_STANDARDS.md Section 7 for authentication patterns

---

## ‚úÖ PHASE 4: VERIFICATION & VALIDATION

**Goal:** Prove that tests work through automated execution and visual evidence.

### Step 4.1: Execute Tests

**Run tests and capture output:**

```bash
# Run all tests
npx playwright test

# Run specific module
npx playwright test tests/auth/auth.spec.ts

# Run with UI mode for debugging
npx playwright test --ui

# Generate HTML report
npx playwright show-report
```

### Step 4.2: Browser Recording (Proof of Work)

**Use `browser_subagent` to:**

1. **Record test execution:**
   ```
   Task: "Navigate to [APP_URL], execute the login flow as defined in TC001,
   record the entire interaction, and capture screenshots of key steps."
   RecordingName: "login_flow_demo"
   ```

2. **Capture success states:**
   - Successful login (dashboard visible)
   - Successful CRUD operations
   - Proper error handling

3. **Save recordings** to artifact directory for user review

### Step 4.3: Create Walkthrough Artifact

**File:** `walkthrough.md` (proof of work document)

**Content:**
- Summary of what was implemented
- Test execution results
- Screenshots of passing tests
- Browser recordings embedded
- Known issues or limitations
- Next steps (if any)

**Use markdown features:**
- Embed screenshots: `![Login Success](path/to/screenshot.png)`
- Embed recordings: `![Test Execution](path/to/recording.webp)`
- Use tables for test results
- Use alerts for important notes

---

## üìÑ PHASE 5: DOCUMENTATION & HANDOFF

**Goal:** Provide comprehensive documentation for maintainability.

### Step 5.1: Generate README.md

**Content:**

1. **Project Overview**
   - What application is being tested
   - Test coverage summary

2. **Tech Stack**
   - Playwright, TypeScript, Node.js versions
   - Key dependencies

3. **Setup Instructions**
   ```bash
   # Clone repository
   # Install dependencies
   npm install
   
   # Setup environment variables
   cp .env.example .env
   # Edit .env with actual values
   ```

4. **Execution Commands**
   ```bash
   # Run all tests
   npx playwright test
   
   # Run specific module
   npx playwright test tests/auth/
   
   # Run with UI mode
   npx playwright test --ui
   
   # Generate report
   npx playwright show-report
   ```

5. **Project Structure**
   ```
   src/
   ‚îú‚îÄ‚îÄ components/     # Reusable UI components
   ‚îú‚îÄ‚îÄ pages/          # Page objects
   ‚îú‚îÄ‚îÄ tests/          # Test specs
   ‚îú‚îÄ‚îÄ fixtures/       # Custom fixtures
   ‚îî‚îÄ‚îÄ api/            # API helpers
   ```

6. **Test Modules**
   - List all modules with TC ranges
   - Link to testcases.md

7. **Contributing Guidelines**
   - How to add new tests
   - Coding standards reference

### Step 5.2: Organize Artifacts

**Ensure all artifacts are in place:**

- [ ] `testplan.md` - High-level strategy
- [ ] `testcases.md` - Detailed test cases
- [ ] `walkthrough.md` - Proof of work
- [ ] `README.md` - Setup and execution guide
- [ ] Screenshots and recordings (if applicable)

### Step 5.3: User Handoff Checklist

**Before completing, verify:**

- [ ] All tests are passing
- [ ] Documentation is complete and accurate
- [ ] `.env.example` is provided (no secrets committed)
- [ ] `package.json` has all dependencies
- [ ] `playwright.config.ts` is properly configured
- [ ] Project structure follows standards
- [ ] User can run tests with provided commands
- [ ] Known issues are documented

---

## üöÄ AI-SPECIFIC BEST PRACTICES

### Parallel vs Sequential Tool Execution

**‚úÖ Execute in Parallel (No Dependencies):**
- Creating multiple component files
- Creating multiple page object files
- Reading multiple documentation files
- Searching for different patterns

**‚ùå Execute Sequentially (Has Dependencies):**
- Create fixture ‚Üí then create test (test needs fixture)
- Read file ‚Üí then edit file (need to see content first)
- Create page object ‚Üí then update fixture (fixture imports page)
- Run test ‚Üí then analyze results (need output first)

### Browser Automation Guidelines

**When to use `browser_subagent`:**

‚úÖ **DO use for:**
- Initial application exploration
- Capturing screenshots for documentation
- Recording test execution demos
- Visual verification of UI changes
- Understanding complex user workflows

‚ùå **DON'T use for:**
- Every single test execution (use `npx playwright test` instead)
- Simple file operations
- Code generation
- Reading documentation

### Artifact Management

**Artifact Directory:** `C:\Users\Asus\.gemini\antigravity\brain\<conversation-id>\`

**Best Practices:**
- Create `implementation_plan.md` during PLANNING mode
- Create `walkthrough.md` after VERIFICATION
- Keep artifacts concise and scannable
- Use markdown formatting (tables, alerts, code blocks)
- Embed screenshots and recordings for visual proof

### User Interaction Patterns

**When to use `notify_user`:**

‚úÖ **DO use when:**
- Requesting review of implementation plan
- Asking which module to generate next
- Blocked on user decision (API endpoints, credentials)
- Requesting review of completed work (walkthrough)

‚ùå **DON'T use when:**
- Just providing status updates (use task_boundary instead)
- Asking obvious questions (make reasonable assumptions)
- Every single step (batch questions together)

**Pattern for module generation:**
```
‚úÖ Module X: [Name] (TC###-TC###) generated successfully.

Next modules to generate:
- Module Y: [Name] (X test cases)
- Module Z: [Name] (X test cases)

Which module should I generate next?
```

### Task Boundary Usage

**Update task_boundary to:**
- Communicate progress to user
- Group related tool calls
- Provide transparency

**TaskName should be granular:**
- ‚úÖ "Implementing Authentication Module"
- ‚úÖ "Generating Test Cases for User Management"
- ‚ùå "Creating Tests" (too broad)

---

## ‚ö° WORKFLOW OPTIMIZATION TIPS

### Batch Operations

**Group similar operations:**
```typescript
// Instead of creating files one by one:
// Create all components in parallel
// Create all pages in parallel
// Create all test files for a module in parallel (if independent)
```

### Avoid Redundant Explorations

**Cache application structure:**
- After initial exploration, document the structure
- Reference the documentation for subsequent modules
- Only re-explore if application changes

### Reuse Page Objects Across Modules

**Don't duplicate:**
- If `LoginPage` is used in multiple modules, create it once
- Share components across pages
- Update fixtures once, use everywhere

### Incremental Verification

**Don't wait until the end:**
- Verify each test as you write it
- Run one test at a time during development
- Fix issues immediately before moving forward
- Only run full suite at the end

---

## üîÅ EXECUTION LOOP SUMMARY

**For each module:**

1. **Explore** the feature (if not already done in Phase 1)
2. **Plan** test cases in `testcases.md` (one module at a time)
3. **STOP** - Ask user which module to implement next
4. **Implement** Page Objects/Components (parallel when possible)
5. **Update** Fixtures
6. **Write** Test Specs (one test at a time)
7. **Verify** each test individually
8. **STOP** - Ask user which module to implement next
9. **Repeat** until all modules complete
10. **Document** in `walkthrough.md` with visual proof

---

## üìö REFERENCE DOCUMENTS

**Always consult these before coding:**

- **`AI_TEST_STANDARDS.md`** - Comprehensive coding standards and best practices
- **`createtest_instructions.md`** - Step-by-step test creation workflow
- **`testplan.md`** - High-level test strategy (created in Phase 2)
- **`testcases.md`** - Detailed test specifications (created in Phase 2)

---

## ‚ú® SUCCESS CRITERIA

**A successful test automation project has:**

- [ ] Thorough application exploration with visual documentation
- [ ] Application-specific modules (not generic templates)
- [ ] Complete test documentation (testplan.md, testcases.md)
- [ ] Clean, maintainable code following COM architecture
- [ ] Fixtures-based dependency injection (no manual instantiation)
- [ ] All tests passing with proof (screenshots, recordings)
- [ ] Comprehensive README with setup and execution instructions
- [ ] Organized artifacts for user review
- [ ] User handoff checklist completed

---

**Remember:** Quality > Speed. Incremental > All-at-once. Visual Proof > Claims.